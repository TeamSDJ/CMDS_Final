{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @REVIEW : Read in data\n",
    "\n",
    "# REVIEW:Read in the crawed wiki pages\n",
    "import pickle\n",
    "import six.moves.cPickle as pickle\n",
    "# load data\n",
    "with open(\"pages.dat\", 'rb') as f:\n",
    "    wiki_pages = pickle.load(f)\n",
    "\n",
    "# REVIEW:Read in the site names and site data\n",
    "import pandas as pd\n",
    "sites_data = pd.read_excel('sitedata.xlsx', 'Section',\n",
    "                           index_col='Section_Id', na_values=['NA'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# @REVIEW: Constructing different document string using differnt wiki page content\n",
    "from analysis_package import *\n",
    "\n",
    "# NOTE:use the wiki page summary as document for each site terms\n",
    "wiki_summaries = []\n",
    "for title in sites_data[['stitle']].values:\n",
    "    wiki_summaries.append(\"\".join([attribute['summary']\n",
    "                                   for attribute in wiki_pages[title[0]].values()]))\n",
    "\n",
    "# NOTE:use the wiki page subtile as document for each site terms\n",
    "wiki_subtitles = []\n",
    "for title in sites_data[['stitle']].values:\n",
    "    wiki_subtitles.append(\"。\".join(merge_lists([list(dict(\n",
    "        attribute['sections']).keys()) for attribute in wiki_pages[title[0]].values()])))\n",
    "\n",
    "# NOTE:use the whole wiki page as document for each site terms\n",
    "# including the title, the sub-titles, the summary content and the\n",
    "# contents of each sub-titles.\n",
    "wiki_segs = []\n",
    "for title in sites_data[['stitle']].values:\n",
    "    wiki_segs.append(\n",
    "        \"。\".join(merge_lists(\n",
    "            # + subtitles\n",
    "            [list(dict(attribute['sections']).values()) for attribute in wiki_pages[title[0]].values()]))  # + the contents of each sub-titles\n",
    "    )\n",
    "\n",
    "# NOTE:use the whole wiki page as document for each site terms\n",
    "# including the title, the sub-titles, the summary content and the\n",
    "# contents of each sub-titles.\n",
    "wiki_whole_page = []\n",
    "for title in sites_data[['stitle']].values:\n",
    "    wiki_whole_page.append(\n",
    "        \"。\".join([attribute['summary'] for attribute in wiki_pages[title[0]].values()] +  # title + summary\n",
    "                 merge_lists(\n",
    "            # + subtitles\n",
    "            [list(dict(attribute['sections']).keys()) for attribute in wiki_pages[title[0]].values()] +\n",
    "            [list(dict(attribute['sections']).values()) for attribute in wiki_pages[title[0]].values()]))  # + the contents of each sub-titles\n",
    "    )\n",
    "\n",
    "# NOTE:use the terms that are annotated with links as documents for each\n",
    "# site terms\n",
    "from hanziconv import HanziConv\n",
    "wiki_links = []\n",
    "for title in sites_data[['stitle']].values:\n",
    "    try:\n",
    "        string = HanziConv.toTraditional(\"。\".join(merge_lists(\n",
    "            [attribute['links'] for attribute in wiki_pages[title[0]].values()])))\n",
    "        wiki_links.append(string)\n",
    "    except:\n",
    "        wiki_links.append(\"。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @REVIEW : Data analysis\n",
    "\n",
    "# REVIEW : Supervised and Unsupervised Learning\n",
    "# generate class prior vector, class-term matrix, covariance\n",
    "# matrices of documents and classes,  document vectors and weighted\n",
    "# document vectors.\n",
    "\n",
    "# @NOTE:For input, we gives the index of each training data, the label of each\n",
    "# training data, and the document content of each training data.\n",
    "# Also, the cutoff frequecy for filtering the vocaburary words are\n",
    "# predefined for each kind of documents.\n",
    "documents = [sites_data[['stitle']], sites_data[['xbody']],\n",
    "             pd.DataFrame(wiki_summaries), pd.DataFrame(wiki_subtitles), pd.DataFrame(wiki_segs), pd.DataFrame(wiki_links)]\n",
    "\n",
    "hfcs = [0., 0.05, 0.01, 0.1, 0.01, 0.]\n",
    "lfcs = [0., 0.2, 0.2, 0., 0.2, 0.]\n",
    "# NOTE:Supervised Learning :\n",
    "sup_priors = []\n",
    "sup_condis = []\n",
    "sup_doc_covs = []\n",
    "sup_class_covs = []\n",
    "sup_doc_vecs = []\n",
    "sup_w_doc_vecs = []\n",
    "# Here we supvisedly use classes as labels.\n",
    "for i in range(len(documents)):\n",
    "    prior_table, condi_table, doc_cov_table, class_cov_table, doc_vec_table, w_doc_vec_table = analysis(\n",
    "        pd, sites_data[['stitle']], sites_data[['CAT2']], documents[i], hfcs[i], lfcs[i])\n",
    "    sup_priors.append(prior_table)\n",
    "    sup_condis.append(condi_table)\n",
    "    sup_doc_covs.append(doc_cov_table)\n",
    "    sup_class_covs.append(class_cov_table)\n",
    "    sup_doc_vecs.append(doc_vec_table)\n",
    "    sup_w_doc_vecs.append(w_doc_vec_table)\n",
    "# NOTE:Unsupervised Learning :\n",
    "unsup_priors = []\n",
    "unsup_condis = []\n",
    "unsup_doc_covs = []\n",
    "unsup_class_covs = []\n",
    "unsup_doc_vecs = []\n",
    "unsup_w_doc_vecs = []\n",
    "# Here we un-supvisedly use data index as labels.\n",
    "for i in range(len(documents)):\n",
    "    prior_table, condi_table, doc_cov_table, class_cov_table, doc_vec_table, w_doc_vec_table = analysis(\n",
    "        pd, sites_data[['stitle']], sites_data[['stitle']], documents[i], hfcs[i], lfcs[i])\n",
    "    unsup_priors.append(prior_table)\n",
    "    unsup_condis.append(condi_table)\n",
    "    unsup_doc_covs.append(doc_cov_table)\n",
    "    unsup_class_covs.append(class_cov_table)\n",
    "    unsup_doc_vecs.append(doc_vec_table)\n",
    "    unsup_w_doc_vecs.append(w_doc_vec_table)\n",
    "\n",
    "# XXX:\n",
    "#  index from 1~6 meaning different strings of document as input :\n",
    "#  index = 1 : using title\n",
    "#  index = 2 : using xbody from site database\n",
    "#  index = 3 : using wiki summary\n",
    "#  index = 4 : using wiki subtitles\n",
    "#  index = 5 : using wiki segtions\n",
    "#  index = 6 : using wiki links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# REVIEW: Visualizing and Evaluating the result for explorative analysis.\n",
    "# here, we use several ways to visualize or generate result,\n",
    "# in order to understand if the document vector are reasonable.\n",
    "\n",
    "from visualize_package import *\n",
    "\n",
    "# @NOTE: plot the generated matrix :\n",
    "for class_cov_table in sup_class_covs:\n",
    "    plot_matrix(class_cov_table)\n",
    "for class_cov_table in unsup_class_covs:\n",
    "    plot_matrix(class_cov_table)\n",
    "# @NOTE: compare the value of two matrix of same shape,\n",
    "# and scatter the values in 2D space in order to understand the\n",
    "# relationship of two matrix.\n",
    "compare_table_values(condi_table11, w_doc_vec_table11)\n",
    "compare_table_values(doc_cov_table5, doc_cov_table4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @NOTE: visualize 2D dimension reducted document vectors, in order to see if the embedding of sites are reasonable.\n",
    "# similar sites should be close in the plot and disimilar sites should be\n",
    "# far away in the plot\n",
    "\n",
    "# NOTE:First, we use the conditional probability or importance of terms given document as vector element,\n",
    "# which is obtain during the unsupvervised NB training phase.\n",
    "for i in range(len(unsup_condis)):\n",
    "    site_2d = dimension_reduction(unsup_condis[i].transpose())\n",
    "    plot_word_embedding(plt, site_2d, num=i, labels=sites_data[['CAT2']])\n",
    "plt.show()\n",
    "\n",
    "# NOTE:Second, we use the term count of each document as vector element.\n",
    "for i in range(len(unsup_doc_vecs)):\n",
    "    site_2d = dimension_reduction(unsup_doc_vecs[i].transpose())\n",
    "    plot_word_embedding(plt, site_2d, num=i, labels=sites_data[['CAT2']])\n",
    "plt.show()\n",
    "\n",
    "# NOTE:Third, we use the over-classes-summed-conditional-probability-weighted term counts as vector element,\n",
    "# where each weight on each term is calculated by summing all\n",
    "# supervised-generated conditional probability of term over all classes.\n",
    "# DEBUG: Third and Forth are the same\n",
    "#for i in range(len(sup_w_doc_vecs)):\n",
    "#    site_2d = dimension_reduction(sup_w_doc_vecs[i].transpose())\n",
    "#    plot_word_embedding(plt, site_2d, num=i, labels=sites_data[['CAT2']])\n",
    "#plt.show()\n",
    "\n",
    "# NOTE:Forth, we use the over-document-summed-conditional-probability-weighted term counts as vector element,\n",
    "# where each weights on each term is calculated by summing all\n",
    "# un-supervised-generated conditional probability of term over all documents.\n",
    "for i in range(len(unsup_w_doc_vecs)):\n",
    "    site_2d = dimension_reduction(unsup_w_doc_vecs[i].transpose())\n",
    "    plot_word_embedding(plt, site_2d, num=i, labels=sites_data[['CAT2']])\n",
    "plt.show()\n",
    "\n",
    "# NOTE:Fifth, we use the rows of document covariance matrix generated from\n",
    "# term count as document vector,\n",
    "for i in range(len(unsup_doc_covs)):\n",
    "    site_2d = dimension_reduction(unsup_doc_covs[i].transpose())\n",
    "    plot_word_embedding(plt, site_2d, num=i, labels=sites_data[['CAT2']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# @NOTE:Then, we try to find similar sites for each vector space using K nearset neighborhood,\n",
    "# in order to check if the vector space can gives reasonable similar sites.\n",
    "# By the method below, we were able to check the local structure of each\n",
    "# vector space.\n",
    "k = 100\n",
    "# NOTE:First, we use the conditional probability or importance of terms given document as vector element,\n",
    "# which is obtain during the unsupvervised NB training phase.\n",
    "unsup_condis_neighbors = []\n",
    "for table in unsup_condis:\n",
    "    unsup_condis_neighbors.append(k_nearest_neighbor(table.transpose(), k))\n",
    "# NOTE:Second, we use the term count of each document as vector element.\n",
    "unsup_doc_vecs_neighbors = []\n",
    "for table in unsup_doc_vecs:\n",
    "    unsup_doc_vecs_neighbors.append(k_nearest_neighbor(table.transpose(), k))\n",
    "# NOTE:Third, we use the over-classes-summed-conditional-probability-weighted term counts as vector element,\n",
    "# where each weight on each term is calculated by summing all\n",
    "# supervised-generated conditional probability of term over all classes.\n",
    "sup_w_doc_vecs_neighbors = []\n",
    "for table in sup_w_doc_vecs:\n",
    "    sup_w_doc_vecs_neighbors.append(k_nearest_neighbor(table.transpose(), k))\n",
    "# NOTE:Forth, we use the over-document-summed-conditional-probability-weighted term counts as vector element,\n",
    "# where each weights on each term is calculated by summing all\n",
    "# un-supervised-generated conditional probability of term over all documents.\n",
    "unsup_w_doc_vecs_neighbors = []\n",
    "for table in unsup_w_doc_vecs:\n",
    "    unsup_w_doc_vecs_neighbors.append(k_nearest_neighbor(table.transpose(), k))\n",
    "unsup_doc_covs_neighbors = []\n",
    "for table in unsup_doc_covs:\n",
    "    unsup_doc_covs_neighbors.append(k_nearest_neighbor(table.transpose(), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: how to compare two ranking list ?\n",
    "from measures.rankedlist import *\n",
    "def compare_two_ranking_lists(table1,table2):\n",
    "    import numpy as np\n",
    "    scores = []\n",
    "    for i in range(len(np.array(table1[0]))):\n",
    "        scores.append(RBO.score(np.array(table1)[i].tolist(), np.array(table2)[i].tolist()))\n",
    "    return sum(scores)/len(np.array(table1[0]))\n",
    "\n",
    "\n",
    "def local_similarity(embeddings):\n",
    "    import numpy as np\n",
    "    k = len(embeddings)\n",
    "    scores_matrix = np.zeros((k,k))\n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            scores_matrix[i,j]=compare_two_ranking_lists(embeddings[i],embeddings[j])\n",
    "    return scores_matrix\n",
    "\n",
    "unsup_condis_sim_M = local_similarity(unsup_condis_neighbors)\n",
    "unsup_doc_vecs_sim_M = local_similarity(unsup_doc_vecs_neighbors)\n",
    "unsup_w_doc_vecs_sim_M = local_similarity(unsup_w_doc_vecs_neighbors)\n",
    "sup_w_doc_vecs_sim_M = local_similarity(sup_w_doc_vecs_neighbors)\n",
    "unsup_doc_covs_sim_M = local_similarity(unsup_doc_covs_neighbors)\n",
    "\n",
    "# REVIEW:visualize local similarity matrix of each kind of embeddings\n",
    "def plot_np_matrix(M):\n",
    "    import matplotlib.pylab as plt\n",
    "    import numpy as np\n",
    "    np.fill_diagonal(M, 0.)\n",
    "    plt.imshow(M,interpolation='nearest')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(1)\n",
    "plot_np_matrix(unsup_doc_vecs_sim_M)\n",
    "plt.figure(2)\n",
    "plot_np_matrix(unsup_doc_covs_sim_M)\n",
    "plt.figure(12)\n",
    "plot_np_matrix(unsup_doc_vecs_sim_M-unsup_doc_covs_sim_M)\n",
    "plt.figure(3)\n",
    "plot_np_matrix(unsup_condis_sim_M)\n",
    "#plt.figure(4)\n",
    "#plot_np_matrix(sup_w_doc_vecs_sim_M)\n",
    "plt.figure(5)\n",
    "plot_np_matrix(unsup_w_doc_vecs_sim_M)\n",
    "#plt.figure(45)\n",
    "#plot_np_matrix(sup_w_doc_vecs_sim_M-unsup_w_doc_vecs_sim_M)\n",
    "# using unsupervised weighted doc vec as embedding is the same as supervised weighted doc vec\n",
    "# => the weighted are the same no matter supevising label or not.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for k in range(6):\n",
    "    plt.figure(k)\n",
    "\n",
    "    all_doc_em_sim_M = local_similarity([unsup_doc_covs_neighbors[k],unsup_doc_vecs_neighbors[k],unsup_condis_neighbors[k],unsup_w_doc_vecs_neighbors[k]])\n",
    "    plot_np_matrix(all_doc_em_sim_M)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: analysis of the result:\n",
    "# 1. using title is similar to using wiki link as documents, since they have similar ranking score in term count unweighted vector space,\n",
    "# including using basic term counts as vector and using the by-generated document covariance matrix as vectors.\n",
    "# Both of them have ranking score outlier using conditional matrix as vector space.\n",
    "# 2. In contrast to above, using body, wiki-summary, wiki-subtitles, and wiki-segtions are more similar in their vector space of term count and wighted term count then simple term count generated vector space.\n",
    "# especially using wiki-subtitles and wiki-segtions.\n",
    "# from 1 and 2 we conclude that, wiki link and title have a similar functionality, the weights on term count will effect the vector space a lot.\n",
    "# 3. the information loss by using covariance document vector from simply term count vector have a ranking :\n",
    "# Information loss: wiki-segtions > wiki-summary = wiki-subtitles > wiki-link > title > body\n",
    "# TODO: Since, its reasonable that with larger document, information loss is greater, however,\n",
    "# body have less information loss, which is strange.\n",
    "# Maybe one reason is that the vocaburary set of body is smaller then title,\n",
    "# another reason is that the document matrix of body is already have some low rank property.\n",
    "# 4. the effect adding wight to term count document vector rank from wiki-link to body:\n",
    "#    wiki-link > title > wiki-summary = wiki-segtions > wiki-subtitles > body\n",
    "# 5. the effect of conditional weighted vector space v.s. term count :\n",
    "#    wiki-link > title = wiki-subtitles = wiki-segtions > wiki-summary > body\n",
    "# from 4.5.\n",
    "# maybe terms in body and wiki-summary better present the site then the others\n",
    "# wiki-link and title have much more noisy term then the others\n",
    "# wiki-subtitles have better weighted effect on codtional weighted vector space\n",
    "# wiki-summary have better weighted effect on term count weighted vector space\n",
    "\n",
    "# also,\n",
    "\n",
    "\n",
    "#TODO: using EM algorithm to cluster sites, and show the unsupervised learned class importance terms in order to understand each topic\n",
    "#TODO: using the semi-supervised method as IR HW by EM algorithm, we will use small amount of human-labeled tour related categories from (FB) or (us) as supervised training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
